{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7fc535c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Science Project\n",
    "#Authors: Raphael Ravinet, Jeffrey Giantonio, Kai Penfold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82bfe2d1",
   "metadata": {},
   "source": [
    "# Importing modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd70ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.tree import export_graphviz\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from full_fred.fred import Fred\n",
    "import yfinance as yf\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import seaborn as sns\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.tsa.stattools import adfuller, acf, pacf\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, TimeSeriesSplit,cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import (accuracy_score, brier_score_loss, classification_report, \n",
    "confusion_matrix, mean_squared_error, precision_score, roc_auc_score,make_scorer,log_loss, f1_score, roc_curve, auc)\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from scipy import interpolate\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import optuna\n",
    "from functools import partial\n",
    "from xgboost import XGBClassifier\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from sklearn import metrics\n",
    "from IPython.display import Image\n",
    "from sklearn.tree import plot_tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef692dc",
   "metadata": {},
   "source": [
    "# Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43bb4e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Connecting with Fred API key\n",
    "os.environ[\"FRED_API_KEY\"] = \"40537c7208bc03361ef9a48a5f60b7e0\"\n",
    "\n",
    "fred = Fred()\n",
    "\n",
    "# Checking if the API key is found in the environment\n",
    "api_key_found = fred.env_api_key_found()\n",
    "print(api_key_found) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43b110a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing datasets\n",
    "\n",
    "#Fred Database\n",
    "cpi_df = fred.get_series_df('CPIAUCSL',observation_start=\"1993-02-01\", observation_end=\"2023-07-01\", frequency = 'm')\n",
    "ffrate_df = fred.get_series_df('FEDFUNDS',observation_start=\"1993-02-01\", observation_end=\"2023-07-01\", frequency = 'm')\n",
    "gdp_df = fred.get_series_df('GDPC1',observation_start=\"1993-01-01\", observation_end=\"2023-07-01\", frequency = 'q')\n",
    "unrate_df = fred.get_series_df('UNRATE',observation_start=\"1993-02-01\", observation_end=\"2023-07-01\", frequency = 'm')\n",
    "indpro_df = fred.get_series_df('INDPRO',observation_start=\"1993-02-01\", observation_end=\"2023-07-01\", frequency = 'm')\n",
    "conssen_df = fred.get_series_df('UMCSENT',observation_start=\"1993-02-01\", observation_end=\"2023-07-01\", frequency = 'm')\n",
    "trade_df = fred.get_series_df('BOPGSTB',observation_start=\"1993-02-01\", observation_end=\"2023-07-01\", frequency = 'm')\n",
    "\n",
    "#Yahoo Finance Database\n",
    "spx = yf.Ticker('SPY')\n",
    "vix = yf.Ticker('^VIX')\n",
    "usdol= yf.Ticker('DX-Y.NYB')\n",
    "\n",
    "spx_df = spx.history(period=\"1d\", start=\"1993-02-01\", end='2023-08-01', interval=\"1mo\")\n",
    "vix_df = vix.history(period=\"1d\", start=\"1993-02-01\", end='2023-08-01', interval=\"1mo\")\n",
    "usdol_df = usdol.history(period=\"1d\", start=\"1993-02-01\", end='2023-08-01', interval=\"1mo\")\n",
    "\n",
    "#MSCI Database\n",
    "acwi = pd.read_excel('/Users/raphaelravinet/Downloads/ACWI index.xls')\n",
    "#From Investing\n",
    "comm_index = pd.read_csv('/Users/raphaelravinet/Downloads/Bloomberg Commodity Historical Data.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d5d619",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Open the datasets we have downloaded from Guru Focus. The are all in the same folder and they are all excel files\n",
    "#So, we will bulk import them\n",
    "\n",
    "directory = '/Users/raphaelravinet/Downloads/Code/DS'\n",
    "financial_data2 = {}\n",
    "\n",
    "for file_name in os.listdir(directory):\n",
    "    if file_name.endswith('.xlsx'):  \n",
    "        file_path = os.path.join(directory, file_name)\n",
    "        try:\n",
    "            df = pd.read_excel(file_path, engine='openpyxl')\n",
    "            financial_data2[file_name] = df\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to read {file_name}: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a75e7f",
   "metadata": {},
   "source": [
    "# Preprocessing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3f2ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating dictionaries with the data\n",
    "\n",
    "fred_data = {'cpi' : cpi_df, 'int_rate': ffrate_df, 'gdp' : gdp_df, \n",
    "             'unemployement': unrate_df, 'indpro': indpro_df, \n",
    "             'consumer_sentiment': conssen_df, 'trade': trade_df}\n",
    "financial_data = {'spx': spx_df,'vix': vix_df, 'dollar_index': usdol_df,}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec2f1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#As these datasets have the same structure we will remove the text from the beginning of our file, filter for the relevant dates, and set them as index\n",
    "#We are also resampling our data (as they were daily) and using the mean to do that\n",
    "\n",
    "\n",
    "for file, df in financial_data2.items():\n",
    "    df = df.iloc[:, :-1]\n",
    "    df = df.iloc[4:]\n",
    "    df.columns = ['Date', file.replace('.xlsx', '').replace('_', ' ')]\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "    df.set_index('Date', inplace=True)\n",
    "    df = df[(df.index > pd.Timestamp('1993-01-01')) & (df.index < pd.Timestamp('2023-07-01'))]\n",
    "    df = df.sort_index(ascending=True)\n",
    "    df = df.resample('M').mean()\n",
    "    df.index = df.index.shift(1, freq='MS')\n",
    "    financial_data2[file] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0c2105",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Renaming and converting date column to date format\n",
    "\n",
    "for name, df in fred_data.items():\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    df.set_index('date', inplace=True)\n",
    "    df.rename(columns={'value': name}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf21018",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Doing the same for the financial data\n",
    "\n",
    "for name, df in financial_data.items():\n",
    "    df.index = pd.to_datetime(df.index)\n",
    "    df.index = df.index.tz_localize(None)\n",
    "    df.index = df.index.normalize()\n",
    "    df.rename(columns={'Close': name}, inplace=True)\n",
    "    financial_data[name] = df.shift(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f76c0b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Combining all into one dataframe. Here all the data will go through based on the index\n",
    "#If just one df has a value for that particular row, all others would be NA.\n",
    "\n",
    "all_dataframes = list(fred_data.values()) + list(financial_data.values()) + list(financial_data2.values())\n",
    "combined_df = pd.concat(all_dataframes, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a7b5c4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Checking the name of our columns\n",
    "combined_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a650a58f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Dropping columns we do not need\n",
    "\n",
    "combined_df = combined_df.drop(columns = ['Open', 'High', 'Low', 'Stock Splits', \n",
    "                            'Capital Gains','realtime_start', 'realtime_end'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e672a362",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#VIX, us dollar index and SPY dataframes, all have columns with the same name ('Volume' and 'Dividend'). So if we just\n",
    "# try to drop using \"standard\" method, we will end up dropping all these columns, however \n",
    "#we don't want to drop the volume and dividends column from SPY as we will be using it in our feature engineering later on\n",
    "\n",
    "combined_df = combined_df.loc[:,~combined_df.columns.duplicated()].copy()\n",
    "combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b7faf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_val = combined_df.isnull().sum()\n",
    "print(f\"Number of NaN Values per Column:\\n{nan_val}' \")\n",
    "combined_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4c8178",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting gdp to numeric values, and taking the percentage between the quarterly points we have available \n",
    "\n",
    "combined_df['gdp'] = pd.to_numeric(combined_df['gdp'], errors='coerce')\n",
    "combined_df['gdp'] = combined_df['gdp'].pct_change(periods=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f59f2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Forward Filling gdp\n",
    "\n",
    "combined_df['gdp'] = combined_df['gdp'].fillna(method='ffill')\n",
    "combined_df['gdp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467bee0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting everything to float so we can do some data analysis and perform some operations\n",
    "combined_df = combined_df.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f35b27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Renaming our columns\n",
    "\n",
    "combined_df.rename(columns ={ 'Volume': 'spx_vol', 'S&P 500 Earnings Yield with Forward Estimate ': 'earnings_yield_FE',\n",
    "       'Shiller PE Ratio for the S&P 500': 'Shiller_PE_ratio',\n",
    "       'S&P 500 PE Ratio with Forward Estimate': 'PE_ratio_FE',\n",
    "       'S&P 500 Dividend Yield:' : 'Dividend_yied',\n",
    "       'BofA Merrill Lynch U.S. High Yield Total Return Index': 'High_yield_return'}, inplace= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a4ff90",
   "metadata": {},
   "source": [
    "# Adding the other 2 dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1ccada",
   "metadata": {},
   "outputs": [],
   "source": [
    "#First let's add clean and ad the commodity index and global markets index to our dataframe.\n",
    "#As they have a very different structure, it is easier to deal with them separately and then add it to the combined df\n",
    "\n",
    "acwi.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227c2206",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Slicing acwi as the first rows are text\n",
    "acwi = acwi.iloc[6:-19].copy()\n",
    "acwi.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45b0da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Renaming columns, converting date column to datetime and checking for null values\n",
    "acwi.columns = ['Date','acwi']\n",
    "acwi['Date'] = pd.to_datetime(acwi['Date'])\n",
    "acwi['acwi'] = acwi['acwi'].astype(float)\n",
    "acwi.set_index('Date',inplace = True)\n",
    "start_date = '1993-01-01'\n",
    "end_date = '2023-05-01'\n",
    "acwi_clean = acwi[(acwi.index >= start_date) & (acwi.index <= end_date)].copy()\n",
    "print(f\"NaN values: {acwi_clean['acwi'].isnull().sum()}\")\n",
    "acwi_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b626b71",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#ACWI value is indexed with last date of the month\n",
    "#so we'll shift one day to make it the first day of the following month as the other df start with the 1st day of the month \n",
    "\n",
    "acwi_clean.index = acwi_clean.index + pd.offsets.MonthBegin(1)\n",
    "acwi_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82a6b18",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Now let's deal with the commodities df\n",
    "comm_index.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41dfc332",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Let's set the date as our index and extract Price column. Maybe we could've used %change here.\n",
    "\n",
    "comm_index.set_index('Date', inplace = True)\n",
    "\n",
    "comm_df = comm_index[['Price']]\n",
    "comm_df = comm_df.rename(columns={'Price': 'comm_index'})\n",
    "comm_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce192af0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Commodities dataframe has its values inversed so we'll reverse the DataFrame order. The first rows are values\n",
    "#from 2023 and the last rows are old value.\n",
    "comm_df = comm_df.iloc[::-1].copy()\n",
    "comm_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e710b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting index to datetime format\n",
    "\n",
    "comm_df.index = pd.to_datetime(comm_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758c0dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#date here is in different format, so let's convert to be in the same format\n",
    "\n",
    "def swap_day_month(date):\n",
    "    if date.day > 12:\n",
    "        return pd.Timestamp(year=date.year, month=date.day, day=date.month)\n",
    "    else:\n",
    "        return date\n",
    "\n",
    "comm_df.index = comm_df.index.map(swap_day_month)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc4f36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "comm_df = comm_df.shift(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02bbd05",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Aligning the indexes\n",
    "comm_df.loc[combined_df.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a433c10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Joining the dataframes on their indices\n",
    "combined_df = combined_df.join(comm_df, how='left')\n",
    "combined_df = combined_df.join(acwi_clean, how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400a0b5c",
   "metadata": {},
   "source": [
    "# Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90fbffde",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# combined_df.hist(figsize=(16,10))\n",
    "# plt.subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=0.4, hspace=0.6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1765c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style=\"darkgrid\")\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(combined_df.loc[combined_df.index > '1995-01-01']['spx'], marker='', color='tab:blue', linewidth=2.5, alpha=0.9, label='SPY Index')\n",
    "\n",
    "plt.title(\"SPY Monthly Returns Over Time\", loc='center', fontsize=14, fontweight=0, color='black')\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"SPY Index\")\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b64095",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting VIX and SPY, to visualize periods of high volatility\n",
    "\n",
    "sns.set_theme(style=\"darkgrid\")\n",
    "\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "color = 'tab:blue'\n",
    "ax1.set_xlabel('Year')\n",
    "ax1.set_ylabel('SPY Index', color=color)\n",
    "ax1.plot(combined_df.index, combined_df['spx'], color=color)\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "ax1.legend(['SPY Index'], loc='upper left')\n",
    "\n",
    "ax2 = ax1.twinx()  \n",
    "color = 'tab:orange'\n",
    "ax2.set_ylabel('VIX Level', color=color)  \n",
    "ax2.plot(combined_df.index, combined_df['vix'], color=color)\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "ax2.legend(['VIX'], loc='upper right')\n",
    "\n",
    "\n",
    "plt.title(\"SPY and VIX Monthly Levels Over Time\", loc='center', fontsize=14, fontweight=0, color='black')\n",
    "\n",
    "fig.tight_layout()  \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8f75a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ohlc_df = combined_df['spx'].resample('Y').agg(['first','max', 'min', 'last'])\n",
    "\n",
    "volume_df = combined_df['spx_vol'].resample('Y').sum()\n",
    "\n",
    "colors = ['green' if close >= open_ else 'red' for open_, close in zip(ohlc_df['first'], ohlc_df['last'])]\n",
    "\n",
    "fig = make_subplots(rows=2, cols=1, shared_xaxes=True,\n",
    "                    vertical_spacing=0.03,\n",
    "                    subplot_titles=('SPY Yearly Candlestick Chart', 'Yearly Volume'),\n",
    "                    row_width=[0.2, 0.7])\n",
    "\n",
    "fig.add_trace(go.Candlestick(x=ohlc_df.index,\n",
    "                             open=ohlc_df['first'],\n",
    "                             high=ohlc_df['max'],\n",
    "                             low=ohlc_df['min'],\n",
    "                             close=ohlc_df['last'],\n",
    "                             increasing_line_color='green',\n",
    "                             decreasing_line_color='red'),\n",
    "              row=1, col=1)\n",
    "\n",
    "fig.add_trace(go.Bar(x=volume_df.index, y=volume_df, marker_color=colors), row=2, col=1)\n",
    "\n",
    "fig.update_layout(title='SPY Yearly Candlestick Chart with Volume',\n",
    "                  yaxis_title='SPY Price',\n",
    "                  xaxis_rangeslider_visible=False,\n",
    "                  showlegend=False,\n",
    "                  plot_bgcolor='white')\n",
    "\n",
    "fig.update_xaxes(title_text=\"<b>Year</b>\", row=2, col=1)\n",
    "fig.update_yaxes(title_text=\"<b>Volume</b>\", row=2, col=1)\n",
    "\n",
    "fig.update_layout(margin=dict(t=100, b=100))\n",
    "\n",
    "fig.layout.annotations[0].text = 'SPY Yearly Candlestick Chart'\n",
    "fig.layout.annotations[1].text = 'Yearly Volume'\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c598a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting SPY over time, using boxplots\n",
    "\n",
    "plt.figure(figsize = (20,13))\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('SPY')\n",
    "sns.boxplot(data=combined_df, x=combined_df.index.year, y=combined_df['spx'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a672ed",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb4232e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Making our data available one month ahead, so we do not incur in look ahead bias\n",
    "\n",
    "projected_variables = ['cpi', 'int_rate', 'gdp', 'unemployement', 'indpro',\n",
    "       'consumer_sentiment','trade']\n",
    "\n",
    "for variable in projected_variables:\n",
    "        lagged_column_name = f'{variable}_report'\n",
    "        combined_df[lagged_column_name] = combined_df[variable].shift(2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17cf9e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ff0b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating annual cpi variable with last 12 months change\n",
    "combined_df['cpi_annual'] = combined_df['cpi'].pct_change(periods=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545f0af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f02ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating our technical indicators\n",
    "\n",
    "delta = combined_df['spx'].diff()\n",
    "\n",
    "up = delta.clip(lower=0)\n",
    "down = -1 * delta.clip(upper=0)\n",
    "\n",
    "# Calculate the Exponential Moving Averages (EMA) of the gains and losses\n",
    "ema_up = up.ewm(com=13, adjust=False).mean()\n",
    "ema_down = down.ewm(com=13, adjust=False).mean()\n",
    "\n",
    "# Calculate the Relative Strength (RS)\n",
    "rs = ema_up / ema_down\n",
    "\n",
    "# Calculate the Relative Strength Index (RSI)\n",
    "combined_df['RSI'] = 100 - (100 / (1 + rs))\n",
    "\n",
    "\n",
    "# Short term exponential moving average (EMA)\n",
    "combined_df['EMA_12'] = np.log(combined_df['spx']) - np.log(combined_df['spx']).ewm(span=12).mean()\n",
    "\n",
    "# Long term exponential moving average (EMA)\n",
    "combined_df['EMA_24'] = np.log(combined_df['spx']) - np.log(combined_df['spx']).ewm(span=24).mean()\n",
    "\n",
    "# # MACD line\n",
    "# combined_df['MACD'] = exp1 - exp2\n",
    "\n",
    "# Signal line\n",
    "# combined_df['Signal_Line'] = combined_df['MACD'].ewm(span=9, adjust=False).mean()\n",
    "\n",
    "obv = pd.Series(index=combined_df.index, dtype='float64')\n",
    "obv.iloc[0] = 0\n",
    "\n",
    "# Calculate OBV\n",
    "for i in range(1, len(combined_df)):\n",
    "    if combined_df['spx'].iloc[i] > combined_df['spx'].iloc[i - 1]:\n",
    "        obv.iloc[i] = obv.iloc[i - 1] + combined_df['spx_vol'].iloc[i]\n",
    "    elif combined_df['spx'].iloc[i] < combined_df['spx'].iloc[i - 1]:\n",
    "        obv.iloc[i] = obv.iloc[i - 1] - combined_df['spx_vol'].iloc[i]\n",
    "    else:\n",
    "        obv.iloc[i] = obv.iloc[i - 1]\n",
    "\n",
    "combined_df['OBV'] = obv\n",
    "\n",
    "\n",
    "# Moving Average\n",
    "combined_df['MA_20'] = combined_df['spx'].rolling(window=20).mean()\n",
    "\n",
    "# Standard Deviation\n",
    "combined_df['STD_20'] = combined_df['spx'].rolling(window=20).std()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad98d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e304b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting the log diff of SPY\n",
    "\n",
    "combined_df['spx_diff'] = np.log(combined_df['spx']).diff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2dc4088",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Below are the transformations functions we will apply to our data\n",
    "\n",
    "def percentage_change(column):\n",
    "    return column.pct_change()\n",
    "\n",
    "def simple_diff(column):\n",
    "    return column.diff()\n",
    "\n",
    "def log_diff(column):\n",
    "    return np.log(column).diff()\n",
    "\n",
    "def identity(column):\n",
    "    return column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158ed763",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Columns that we will apply the transformations\n",
    "\n",
    "transformations ={\n",
    "    'cpi_report': percentage_change,\n",
    "    'int_rate_report': simple_diff,\n",
    "    'gdp_report': identity,\n",
    "    'unemployement_report': simple_diff,\n",
    "    'indpro_report': percentage_change,\n",
    "    'consumer_sentiment_report': percentage_change,\n",
    "    'trade_report': percentage_change,\n",
    "    'spx_vol': log_diff,\n",
    "    'vix': percentage_change,\n",
    "    'dollar_index': percentage_change,\n",
    "    'comm_index': percentage_change,\n",
    "    'acwi': percentage_change,\n",
    "    'cpi_annual': simple_diff,\n",
    "    'S&P 500 Dividend Yield': log_diff,\n",
    "    'EMA_12': identity,\n",
    "    'EMA_24': identity,\n",
    "    'earnings_yield_FE': percentage_change,\n",
    "    'Shiller_PE_ratio': percentage_change,\n",
    "    'PE_ratio_FE': percentage_change,\n",
    "    'High_yield_return': percentage_change,\n",
    "    'OBV': identity,\n",
    "    'RSI': identity,\n",
    "    'spx': identity,\n",
    "    'spx_diff': identity,\n",
    "    \n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019c9a9b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "transformed_columns = {}\n",
    "for column, transformation in transformations.items():\n",
    "    transformed_columns[column] = transformation(combined_df[column])\n",
    "\n",
    "# Combine the results into a new DataFrame\n",
    "transformed_df = pd.DataFrame(transformed_columns)\n",
    "transformed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa54271",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting the name of the original features, before lagging some of them\n",
    "\n",
    "original_features = transformed_df.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48956972",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking which columns we didn't carry it over to transformed df\n",
    "\n",
    "set(list(combined_df.columns)) - set(list(transformed_df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af91fbd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# #Checking how correlated the variables are amongst themselves\n",
    "\n",
    "# plt.figure(figsize=(24,18))\n",
    "# features_corr = sns.heatmap(transformed_df.corr(), cmap = 'RdYlGn', annot = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f61b7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fcd4927",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Taking the log difference between tomorrow and today's return\n",
    "\n",
    "transformed_df['log_ret_spy'] = np.log(transformed_df['spx']).diff().shift(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f253be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating our target variable. One without threshold and the other one with threshold.\n",
    "threshold = 0.03\n",
    "transformed_df['y_new'] = transformed_df['log_ret_spy'].apply(lambda x: '1' if x > threshold else '0').astype(int)\n",
    "transformed_df['y'] = (transformed_df['log_ret_spy'] >= 0).astype(int)\n",
    "\n",
    "\n",
    "#Creating our weight variable based on absolute return\n",
    "\n",
    "transformed_df['weight'] = abs(transformed_df['log_ret_spy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fde5f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lagging our variables\n",
    "variables_to_lag = ['cpi_report', 'int_rate_report', 'gdp_report', 'unemployement_report', 'indpro_report',\n",
    "       'consumer_sentiment_report', 'spx_diff']\n",
    "\n",
    "max_lag = 5\n",
    "for variable in variables_to_lag:\n",
    "    for lag in range(1,max_lag + 1):\n",
    "        lagged_column_name = f'{variable}_lag{lag}'\n",
    "        transformed_df[lagged_column_name] = transformed_df[variable].shift(lag)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06aff7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filtering our dataframe for the period we will be analysing \n",
    "\n",
    "transformed_df = transformed_df.loc[(transformed_df.index > '1995-01-01') & (transformed_df.index < '2023-05-01')].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64317d74",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Our final dataframe\n",
    "print(transformed_df.isnull().sum().sum())\n",
    "transformed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69ddd05",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f030258",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting into Train and test. Here we are leaving the last 40 observations for testing.\n",
    "#Not that this is not our cross validation split, we will further split our train set here into multiple train and\n",
    "#tests.\n",
    "transformed_df_train = transformed_df.iloc[:-40].copy()\n",
    "transformed_df_test = transformed_df.iloc[-40:].copy() \n",
    "transformed_df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9d1cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565de48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_to_plot = ['cpi_report', 'int_rate_report', 'gdp_report', 'unemployement_report', 'indpro_report',\n",
    "       'consumer_sentiment_report', 'trade_report', 'spx_vol', 'dollar_index','comm_index', 'y_new']\n",
    "features_to_plot_df = transformed_df[features_to_plot]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dacc3778",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_to_plot_mapping = {\n",
    "    'cpi_report': 'CPI',\n",
    "    'comm_index': 'Commodity_index',\n",
    "    'consumer_sentiment_report': 'Sentiment_index',\n",
    "    'dollar_index': 'Dollar_index',\n",
    "    'gdp_report': 'GDP',\n",
    "    'indpro_report': 'Ind_production',\n",
    "    'int_rate_report': 'Int_rate',\n",
    "    'spx_vol': 'SPY Volume',\n",
    "    'trade_report': 'Trade Balance',\n",
    "    'unemployement_report': 'Unemployement_rate',}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7d3b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_to_plot_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e292bf",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X_to_plot = features_to_plot_df.drop('y_new', axis=1)\n",
    "y_to_plot = features_to_plot_df['y_new']\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_to_plot)\n",
    "\n",
    "X_scaled_df = pd.DataFrame(X_scaled, columns=X_to_plot.columns, index=X_to_plot.index)\n",
    "X_scaled_df['y_new'] = y_to_plot\n",
    "\n",
    "X_scaled_df_renamed = X_scaled_df.rename(columns=feature_to_plot_mapping)\n",
    "\n",
    "\n",
    "y_min = X_scaled_df.drop('y_new', axis=1).min().min() - 1\n",
    "y_max = X_scaled_df.drop('y_new', axis=1).max().max() + 1\n",
    "\n",
    "grouped_renamed = X_scaled_df_renamed.groupby('y_new')\n",
    "for name, group in grouped_renamed:\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))  \n",
    "    group.drop('y_new', axis=1).boxplot(rot=90, ax=ax)\n",
    "    ax.set_ylim(y_min, y_max)  \n",
    "    plt.title(f'Boxplot for class {name}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92f46a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking how correlated the variables are amongst themselves\n",
    "\n",
    "plt.figure(figsize=(16,10))\n",
    "features_corr = sns.heatmap(X_scaled_df_renamed.corr(), cmap = 'RdYlGn', annot = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9a6146",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting the features we will use as our independent variable\n",
    "features =['cpi_report', 'int_rate_report', 'gdp_report', 'unemployement_report',\n",
    "       'indpro_report', 'consumer_sentiment_report', 'trade_report', 'spx_vol',\n",
    "       'dollar_index', 'comm_index', 'acwi', 'cpi_annual',\n",
    "       'S&P 500 Dividend Yield', 'EMA_12', 'EMA_24', 'earnings_yield_FE',\n",
    "       'Shiller_PE_ratio', 'PE_ratio_FE', 'OBV', 'RSI',\n",
    "       'spx_diff','cpi_report_lag1', 'cpi_report_lag2',\n",
    "       'cpi_report_lag3', 'cpi_report_lag4', 'cpi_report_lag5',\n",
    "       'int_rate_report_lag1', 'int_rate_report_lag2', 'int_rate_report_lag3',\n",
    "       'int_rate_report_lag4', 'int_rate_report_lag5', 'gdp_report_lag1',\n",
    "       'gdp_report_lag2', 'gdp_report_lag3', 'gdp_report_lag4',\n",
    "       'gdp_report_lag5', 'unemployement_report_lag1',\n",
    "       'unemployement_report_lag2', 'unemployement_report_lag3',\n",
    "       'unemployement_report_lag4', 'unemployement_report_lag5',\n",
    "       'indpro_report_lag1', 'indpro_report_lag2', 'indpro_report_lag3',\n",
    "       'indpro_report_lag4', 'indpro_report_lag5',\n",
    "       'consumer_sentiment_report_lag1', 'consumer_sentiment_report_lag2',\n",
    "       'consumer_sentiment_report_lag3', 'consumer_sentiment_report_lag4',\n",
    "       'consumer_sentiment_report_lag5', 'spx_diff_lag1', 'spx_diff_lag2',\n",
    "       'spx_diff_lag3', 'spx_diff_lag4', 'spx_diff_lag5',\n",
    "          'High_yield_return']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5175694b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining X variable according to our features\n",
    "X = transformed_df_train[features]\n",
    "X_test = transformed_df_test[features]\n",
    "\n",
    "#Defining weight variable for train and test set\n",
    "weight_train = transformed_df_train['weight']\n",
    "weight_test = transformed_df_test['weight']\n",
    "\n",
    "#Defining Y variable for train and test set\n",
    "y_new = transformed_df_train['y_new']\n",
    "y_new_test = transformed_df_test['y_new']\n",
    "y = transformed_df_train['y']\n",
    "y_test = transformed_df_test['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90089dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating graph for monthly return distribution\n",
    "positive_above_3 = transformed_df[transformed_df['y_new'] == 1].copy()\n",
    "\n",
    "positive_above_3['Year'] = positive_above_3.index.year.copy()\n",
    "\n",
    "grouped = positive_above_3.groupby('Year').size().reset_index(name='counts').copy()\n",
    "\n",
    "fig = go.Figure(data=[\n",
    "    go.Bar(name='Positive Above 3%', x=grouped['Year'], y=grouped['counts'], marker_color='green')\n",
    "])\n",
    "\n",
    "fig.update_layout(title='Distribution of Monthly Returns Above 3% for S&P 500 Over Time',\n",
    "                  xaxis_title='Year', yaxis_title='Number of Months')\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c218fdc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#This will display our class imbalance\n",
    "\n",
    "classes, counts = np.unique(y_to_plot, return_counts=True)\n",
    "\n",
    "classes = ['Below 3%', 'Above 3%']\n",
    "counts_list = [counts[0], counts[1]]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "ax = sns.barplot(x=classes, y=counts_list, palette=['#e41a1c', '#4daf4a'])\n",
    "\n",
    "for i, count in enumerate(counts_list):\n",
    "    ax.text(i, count, str(count), ha='center', va='bottom')\n",
    "\n",
    "plt.title('Months with returns above or below 3%')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35fca71",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Checking if the previous return of S&P500 influence the current returns \n",
    "\n",
    "plot_acf(transformed_df['log_ret_spy'], lags=20) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5959feda",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_pacf(transformed_df['log_ret_spy'], lags=20)\n",
    "plt.title('Partial Autocorrelation Log_ret_SPY')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc7e29c",
   "metadata": {},
   "source": [
    "# Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6235f035",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a dictionary with the models we will use. We will pass this on to the cross validation function\n",
    "\n",
    "binary_models = {\n",
    "    'LogisticRegression': LogisticRegression,\n",
    "    'RandomForest': RandomForestClassifier,\n",
    "    'GradientBoosting': GradientBoostingClassifier,\n",
    "    'XGB': XGBClassifier\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e654e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining our cross validation function. In this function we will pass the models we want to test, our X and y\n",
    "#our weights and the number of splits in which we will split our data into training and test sets.\n",
    "#We also have an optional argument which is the parameters that we may pass to our models.\n",
    "#We will run first to get our baseline model\n",
    "\n",
    "def time_series_validation_binary(model_classes, X, y, n_splits,weights = None,random_state = 50, model_configs=None):\n",
    "    model_scores = {}\n",
    "    feature_importance_dfs = {}\n",
    "    y_proba_cv = {}\n",
    "    roc_data = {}\n",
    "\n",
    "    tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "\n",
    "    for name, model_class in model_classes.items():\n",
    "        config = model_configs.get(name, {}) if model_configs is not None else {}\n",
    "        config['random_state'] = random_state\n",
    "\n",
    "        # Initialize metrics\n",
    "        accuracies = []\n",
    "        log_losses = []\n",
    "        importances = []\n",
    "        all_importances = []\n",
    "        aggregated_cm = np.zeros((2, 2), dtype=int)\n",
    "        auc_scores = []\n",
    "        y_proba_model = []\n",
    "        split_indices = [] \n",
    "        roc_data[name] = {'fpr': [], 'tpr': [], 'auc': []}  \n",
    "\n",
    "        for train_index, test_index in tscv.split(X):\n",
    "            split_indices.append((train_index, test_index))\n",
    "            X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "            y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "            sample_weight_train = weights.iloc[train_index] if weights is not None else None\n",
    "            sample_weight_test = weights.iloc[test_index] if weights is not None else None\n",
    "\n",
    "            # Scaling the data\n",
    "            scaler = StandardScaler()\n",
    "            X_train_scaled = scaler.fit_transform(X_train)\n",
    "            X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "            # Fitting the model - using parameters if we pass them on\n",
    "            model = model_class(**config)\n",
    "            model.fit(X_train_scaled, y_train, sample_weight=sample_weight_train)\n",
    "            if name == 'RandomForest':  \n",
    "                estimator = model.estimators_[0]\n",
    "                \n",
    "                export_graphviz(estimator, out_file=f'{name}_tree.dot', \n",
    "                                feature_names=X.columns,\n",
    "                                rounded=True, proportion=False, \n",
    "                                precision=2, filled=True)\n",
    "                os.system(f'dot -Tpng {name}_tree.dot -o {name}_tree.png')\n",
    "\n",
    "        \n",
    "            \n",
    "            y_proba = model.predict_proba(X_test_scaled)\n",
    "            y_proba_model.append(y_proba[:, 1])\n",
    "            \n",
    "            fpr, tpr, thresholds = roc_curve(y_test, y_proba[:, 1], sample_weight=sample_weight_test)\n",
    "            roc_auc = auc(fpr, tpr)\n",
    "\n",
    "            # Storing feature importances if available - not all models would get that\n",
    "            if hasattr(model, 'feature_importances_'):\n",
    "                all_importances.append(model.feature_importances_)\n",
    "\n",
    "            # Calculate and store evaluation metrics - we might not need to use log loss\n",
    "            log_loss_score = log_loss(y_test, y_proba, sample_weight=sample_weight_test)\n",
    "            log_losses.append(log_loss_score)\n",
    "            accuracies.append(accuracy_score(y_test, model.predict(X_test_scaled)))\n",
    "            auc_score = roc_auc_score(y_test, y_proba[:, 1], sample_weight=sample_weight_test)\n",
    "            auc_scores.append(auc_score)\n",
    "            roc_data[name]['fpr'].append(fpr)\n",
    "            roc_data[name]['tpr'].append(tpr)\n",
    "            roc_data[name]['auc'].append(roc_auc)\n",
    "\n",
    "            # Update the aggregated confusion matrix\n",
    "            cm = confusion_matrix(y_test, model.predict(X_test_scaled), labels=[0, 1])\n",
    "            aggregated_cm += cm\n",
    "\n",
    "        # Calculate average metrics\n",
    "        avg_log_loss = np.mean(log_losses)\n",
    "        avg_accuracy = np.mean(accuracies)\n",
    "        avg_auc = np.mean(auc_scores)\n",
    "\n",
    "        # Aggregate feature importances and store in DataFrame\n",
    "        if all_importances:\n",
    "            avg_importance = np.mean(all_importances, axis=0)\n",
    "            feature_importance_df = pd.DataFrame({\n",
    "                'Feature': X.columns,\n",
    "                f'{name}_Importance': avg_importance\n",
    "            })\n",
    "            feature_importance_dfs[name] = feature_importance_df\n",
    "\n",
    "        # Store the final results\n",
    "        model_scores[name] = {\n",
    "            'Average Accuracy': avg_accuracy,\n",
    "            'Average Log Loss': avg_log_loss,\n",
    "            'Average AUC': avg_auc,\n",
    "            'Aggregated Confusion Matrix': aggregated_cm\n",
    "        }\n",
    "        y_proba_cv[name] = y_proba_model\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "    return model_scores, feature_importance_dfs, y_proba_cv, split_indices, roc_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71a5f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Storing the results from CV\n",
    "\n",
    "scores, feature_importances, y_proba_cvv,split_indices, roc_data = time_series_validation_binary(\n",
    "    model_classes=binary_models, \n",
    "    X=X, \n",
    "    y=y_new,\n",
    "    n_splits= 5,\n",
    "    weights = weight_train\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b796942d",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e730ed5e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Storing roc data for ROC curve graph\n",
    "rf_rocdata = roc_data['RandomForest']\n",
    "lr_rocdata = roc_data['LogisticRegression']\n",
    "gb_rocdata = roc_data['GradientBoosting']\n",
    "xgb_rocdata = roc_data['XGB']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5426a4bd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Displaying from Random Forest tree\n",
    "\n",
    "Image(filename='RandomForest_tree.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bdf5b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting our cross validation splits.\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 7))\n",
    "\n",
    "\n",
    "train_color = '#1f77b4'  \n",
    "test_color = '#ff7f0e'   \n",
    "\n",
    "for i, (train_indices, test_indices) in enumerate(split_indices):\n",
    "    ax.barh(y=i, width=max(train_indices)-min(train_indices), left=min(train_indices), \n",
    "            color=train_color, height=0.4, label='Train' if i == 0 else \"\")\n",
    "    ax.barh(y=i, width=max(test_indices)-min(test_indices), left=min(test_indices), \n",
    "            color=test_color, height=0.4, label='Test' if i == 0 else \"\")\n",
    "\n",
    "ax.legend(fontsize='large')\n",
    "ax.set_yticks(range(len(split_indices)))\n",
    "ax.set_yticklabels([f'CV {i+1}' for i in range(len(split_indices))])\n",
    "ax.set_xlabel('Sample Index', fontsize='large', fontweight='bold')\n",
    "ax.set_ylabel('CV Iteration', fontsize='large', fontweight='bold')\n",
    "ax.set_title('Time Series Split', fontsize='x-large', fontweight='bold')\n",
    "\n",
    "ax.invert_yaxis()\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2609b710",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Storing results and feature importance from the models\n",
    "random_forest_scores = scores['RandomForest']\n",
    "random_forest_feature_importances = feature_importances['RandomForest']\n",
    "gradient_boosting_scores = scores['GradientBoosting']\n",
    "gradient_boosting_feature_importances = feature_importances['GradientBoosting']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63d6098",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Checking top 10 feature importances from each of our models\n",
    "\n",
    "df_rf_top10 = feature_importances['RandomForest'].nlargest(10, 'RandomForest_Importance')\n",
    "df_gb_top10 = feature_importances['GradientBoosting'].nlargest(10, 'GradientBoosting_Importance')\n",
    "df_xgb_top10 = feature_importances['XGB'].nlargest(10, 'XGB_Importance')\n",
    "\n",
    "df_rf_top10.reset_index(drop=True, inplace=True)\n",
    "df_gb_top10.reset_index(drop=True, inplace=True)\n",
    "df_xgb_top10.reset_index(drop=True, inplace=True)\n",
    "df_top_features = pd.concat([df_rf_top10, df_gb_top10, df_xgb_top10], axis=1)\n",
    "\n",
    "df_top_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0254b2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Plotting the confusion matrices\n",
    "\n",
    "cm_matrix_rf = random_forest_scores['Aggregated Confusion Matrix']\n",
    "\n",
    "sns.heatmap(cm_matrix_rf, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix for Random Forest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75b1df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confusion matrix GB\n",
    "\n",
    "cm_matrix_rf = gradient_boosting_scores['Aggregated Confusion Matrix']\n",
    "\n",
    "sns.heatmap(cm_matrix_rf, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix for Gradient Boosting')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904b3584",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we have our hyperparameter optimization parameters to test. We will be using optuna to do that. \n",
    "\n",
    "\n",
    "\n",
    "def xgb_param_sample(trial):\n",
    "    params = {\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.1, 1.0),\n",
    "        'learning_rate': trial.suggest_float(\"learning_rate\", 0.001, 0.15, log=True),\n",
    "        'max_depth': trial.suggest_int('max_depth', 1, 40),\n",
    "        'subsample': trial.suggest_float('subsample', 0.1, 1.0),\n",
    "        'gamma': trial.suggest_float(\"gamma\", 0, 2.0),\n",
    "        'reg_lambda': trial.suggest_float(\"reg_lambda\", 1e-8, 100.0, log=True),\n",
    "        'reg_alpha': trial.suggest_float(\"reg_alpha\", 1e-8, 100.0, log=True),\n",
    "        'min_child_weight': trial.suggest_int(\"min_child_weight\", 1, 20),\n",
    "        'n_estimators': trial.suggest_int(\"n_estimators\", 700, 1200),\n",
    "\n",
    "        # some fixed hyperparameters\n",
    "        'objective': 'binary:logistic',\n",
    "        'eval_metric': 'auc',\n",
    "    }\n",
    "\n",
    "    return params\n",
    "\n",
    "def random_forest_param_sample(trial):\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int(\"n_estimators\", 400, 500),\n",
    "        'max_depth': trial.suggest_int(\"max_depth\", 1, 100),\n",
    "        'min_samples_split': trial.suggest_float(\"min_samples_split\", 0.0000001, 0.5, log=True),\n",
    "        'min_samples_leaf': trial.suggest_float(\"min_samples_leaf\", 0.0000001, 0.5, log=True),\n",
    "        'max_features': trial.suggest_float('max_features', 0.0, 1.0),\n",
    "        'min_impurity_decrease': trial.suggest_float(\"min_impurity_decrease\", 0.0000001, 1, log=True),\n",
    "        'ccp_alpha': trial.suggest_float(\"ccp_alpha\", 0.0000001, 2, log=True),\n",
    "        'criterion': trial.suggest_categorical(\"criterion\", ['gini', 'entropy', 'log_loss']),\n",
    "        'n_jobs':trial.suggest_categorical('n_jobs',[-1]),\n",
    "        'oob_score':trial.suggest_categorical('oob_score',[False]),\n",
    "        'class_weight': trial.suggest_categorical('class_weight',[None])\n",
    " \n",
    "    }\n",
    "    \n",
    "    return params\n",
    "\n",
    "def gradient_boosting_param_sample(trial):\n",
    "    params = {\n",
    "        'learning_rate': trial.suggest_float(\"learning_rate\", 0.001, 0.15, log=True),\n",
    "        'n_estimators': trial.suggest_int(\"n_estimators\", 100, 1000),\n",
    "        'max_depth': trial.suggest_int('max_depth', 1, 40),\n",
    "        'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),\n",
    "        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 20),\n",
    "        'subsample': trial.suggest_float('subsample', 0.1, 1.0),\n",
    "        'max_features': trial.suggest_categorical('max_features', ['sqrt', 'log2', None])\n",
    "    }\n",
    "\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70cf8339",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Random Forest objective function and study\n",
    "\n",
    "def random_forest_objective_auc(trial):\n",
    "    params = random_forest_param_sample(trial)\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "    auc_scores = []\n",
    "\n",
    "    for train_index, test_index in tscv.split(X):\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        weighttrain = weight_train.iloc[train_index]\n",
    "        weighttest = weight_train.iloc[test_index]\n",
    "\n",
    "        model = RandomForestClassifier(**params)\n",
    "        model.fit(X_train, y_train, sample_weight=weighttrain)\n",
    "\n",
    "        preds = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "\n",
    "        score = roc_auc_score(y_test, preds, sample_weight=weighttest)\n",
    "        if not np.isnan(score):\n",
    "            auc_scores.append(score)\n",
    "    \n",
    "    if not auc_scores:\n",
    "        return np.nan\n",
    "\n",
    "    return np.mean(auc_scores)\n",
    "\n",
    "\n",
    "rf_auc_study = optuna.create_study(direction='maximize')\n",
    "rf_auc_study.optimize(random_forest_objective_auc, n_trials=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd9f1ba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# XGB objective function and study\n",
    "\n",
    "def xgb_objective_auc(trial):\n",
    "    params = xgb_param_sample(trial)\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "    auc_scores = []\n",
    "\n",
    "    for train_index, test_index in tscv.split(X):\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        weighttrain = weight_train.iloc[train_index]\n",
    "        weighttest = weight_train.iloc[test_index]\n",
    "\n",
    "        model = XGBClassifier(**params)\n",
    "        model.fit(X_train, y_train, sample_weight=weighttrain)\n",
    "\n",
    "        preds = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "\n",
    "        score = roc_auc_score(y_test, preds, sample_weight=weighttest)\n",
    "        if not np.isnan(score):\n",
    "            auc_scores.append(score)\n",
    "    \n",
    "    if not auc_scores:\n",
    "        return np.nan\n",
    "\n",
    "    return np.mean(auc_scores)\n",
    "\n",
    "\n",
    "xgb_auc_study = optuna.create_study(direction='maximize')\n",
    "xgb_auc_study.optimize(xgb_objective_auc, n_trials=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa167bcc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Gradient Boosting objective function and study\n",
    "\n",
    "def gradient_boosting_objective_auc(trial):\n",
    "    params = gradient_boosting_param_sample(trial)\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "    auc_scores = []\n",
    "\n",
    "    for train_index, test_index in tscv.split(X):\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        weighttrain = weight_train.iloc[train_index]\n",
    "        weighttest = weight_train.iloc[test_index]\n",
    "\n",
    "        model = GradientBoostingClassifier(**params)\n",
    "        model.fit(X_train, y_train, sample_weight=weighttrain)\n",
    "\n",
    "        preds = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "\n",
    "        score = roc_auc_score(y_test, preds, sample_weight=weighttest)\n",
    "        if not np.isnan(score):\n",
    "            auc_scores.append(score)\n",
    "    \n",
    "    if not auc_scores:\n",
    "        return np.nan\n",
    "\n",
    "    return np.mean(auc_scores)\n",
    "\n",
    "\n",
    "gb_auc_study = optuna.create_study(direction='maximize')\n",
    "gb_auc_study.optimize(gradient_boosting_objective_auc, n_trials=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e78b8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Storing best scores and best params\n",
    "\n",
    "rf_best_score = rf_auc_study.best_value\n",
    "# xgb_best_score_auc = xgb_auc_study.best_value\n",
    "gb_best_score_auc = gb_auc_study.best_value\n",
    "\n",
    "rf_best_params_auc = rf_auc_study.best_params\n",
    "# xgb_best_params_auc = xgb_auc_study.best_params\n",
    "gb_best_params_auc = gb_auc_study.best_params\n",
    "\n",
    "# Creating dictionary with each model's best parameters\n",
    "model_configs = {'RandomForest': rf_best_params_auc}\n",
    "# model_configs['XGB'] = xgb_best_params_auc\n",
    "model_configs['GradientBoosting'] = gb_best_params_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b214954b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Parameter importance Random Forest\n",
    "optuna.visualization.plot_param_importances(rf_auc_study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53cee57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameter importance Gradient Boosting\n",
    "optuna.visualization.plot_param_importances(gb_auc_study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa555ee7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#RF optm history\n",
    "\n",
    "optuna.visualization.plot_optimization_history(rf_auc_study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aba97cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#GB optm history\n",
    "\n",
    "optuna.visualization.plot_optimization_history(gb_auc_study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50daccdd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Displaying RF importance in sample\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "plt.figure(figsize=(8, 6))\n",
    "top_n = 20\n",
    "top_features_df = random_forest_feature_importances.sort_values(by='RandomForest_Importance', ascending=False).head(top_n)\n",
    "sns.barplot(x='RandomForest_Importance', y='Feature', data=top_features_df)\n",
    "\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.tight_layout() \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc072835",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Displaying GB importance \n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "plt.figure(figsize=(8, 6))\n",
    "top_n = 20\n",
    "top_features_df_gb = gradient_boosting_feature_importances.sort_values(by='GradientBoosting_Importance', ascending=False).head(top_n)\n",
    "sns.barplot(x='GradientBoosting_Importance', y='Feature', data=top_features_df_gb)\n",
    "\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.tight_layout() \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8dc6e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is our out of sample evaluation. It will be performed in the test set we held out.\n",
    "\n",
    "def out_of_sample_evaluation(model_classes, X_train, y_train, X_test, y_test, random_state=1, model_configs=None, use_sample_weights=False, weight_train=None, weight_test=None):\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    results = {}\n",
    "    y_proba_out_of_sample = {}\n",
    "    trained_models = {}\n",
    "    feature_importancess = {}\n",
    "\n",
    "    for name, model_class in model_classes.items():\n",
    "        config = model_configs.get(name, {}) if model_configs is not None else {}\n",
    "        config['random_state'] = random_state\n",
    "\n",
    "        model = model_class(**config)\n",
    "        if use_sample_weights:\n",
    "            model.fit(X_train_scaled, y_train, sample_weight=weight_train)\n",
    "        else:\n",
    "            model.fit(X_train_scaled, y_train)\n",
    "\n",
    "        if name == 'RandomForest':\n",
    "            feature_names = X_train.columns if isinstance(X_train, pd.DataFrame) else [f'feature_{i}' for i in range(X_train.shape[1])]\n",
    "            estimator = model.estimators_[0]\n",
    "            \n",
    "            export_graphviz(estimator, out_file=f'{name}_tree.dot', \n",
    "                            feature_names=feature_names,\n",
    "                            rounded=True, proportion=False, \n",
    "                            precision=2, filled=True)\n",
    "            os.system(f'dot -Tpng {name}_tree.dot -o {name}_tree.png')\n",
    "\n",
    "        trained_models[name] = model\n",
    "        \n",
    "        if name == 'LogisticRegression':\n",
    "            if isinstance(X_train, pd.DataFrame):\n",
    "                feature_names = X_train.columns\n",
    "            else:\n",
    "                feature_names = [f'feature_{i}' for i in range(X_train.shape[1])]\n",
    "            \n",
    "            coefficients = model.coef_[0]\n",
    "        \n",
    "            feature_importance = dict(zip(feature_names, coefficients))\n",
    "            feature_importancess[name] = feature_importance\n",
    "\n",
    "\n",
    "        y_proba = model.predict_proba(X_test_scaled)[:, 1]\n",
    "        y_proba_out_of_sample[name] = y_proba.tolist()\n",
    "\n",
    "        y_pred_custom_threshold = (y_proba > 0.5).astype(int)\n",
    "\n",
    "        test_accuracy = accuracy_score(y_test, y_pred_custom_threshold)\n",
    "        test_auc = roc_auc_score(y_test, y_proba)\n",
    "        test_log_loss = log_loss(y_test, y_proba, sample_weight=weight_test)\n",
    "        test_cm = confusion_matrix(y_test, y_pred_custom_threshold)\n",
    "\n",
    "        results[name] = {\n",
    "            'Average Accuracy': test_accuracy,\n",
    "            'Average Log Loss': test_log_loss,\n",
    "            'Average AUC': test_auc,\n",
    "            'Aggregated Confusion Matrix': test_cm.tolist(), \n",
    "        }\n",
    "\n",
    "    return results, y_proba_out_of_sample, trained_models, feature_importancess\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9768b617",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Results with 50% probability cutoff\n",
    "\n",
    "results, y_proba_out_of_sample,trained_models, feature_importances_out_of_sample = out_of_sample_evaluation(\n",
    "    model_classes=binary_models,\n",
    "    X_train=X,\n",
    "    y_train=y_new,\n",
    "    X_test=X_test,\n",
    "    y_test=y_new_test,\n",
    "    use_sample_weights=True,\n",
    "    weight_train=weight_train,\n",
    "    weight_test=weight_test,\n",
    "    model_configs=model_configs\n",
    ")\n",
    "results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac52fe34",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Results 40% probability cutoff\n",
    "results_40, y_proba_out_of_sample_40,trained_models_40, feature_importances_out_of_sample_40 = out_of_sample_evaluation(\n",
    "    model_classes=binary_models,\n",
    "    X_train=X,\n",
    "    y_train=y_new,\n",
    "    X_test=X_test,\n",
    "    y_test=y_new_test,\n",
    "    use_sample_weights=True,\n",
    "    weight_train=weight_train,\n",
    "    weight_test=weight_test,\n",
    "    model_configs=model_configs\n",
    ")\n",
    "results_40\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1db970",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cm_matrix_rf_out_of_sample = results['GradientBoosting']['Aggregated Confusion Matrix']\n",
    "\n",
    "sns.heatmap(cm_matrix_rf_out_of_sample, annot=True, fmt='d', cmap='Blues')\n",
    "\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix for Gradient Boosting: 50% Probability Cutoff')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ac6762",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cm_matrix_rf_out_of_sample_40 = results_40['GradientBoosting']['Aggregated Confusion Matrix']\n",
    "\n",
    "sns.heatmap(cm_matrix_rf_out_of_sample, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix for Random Forest: 40% Probability Cutoff')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f83fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_matrix_rf_out_of_sample_40 = results_40['RandomForest']['Aggregated Confusion Matrix']\n",
    "\n",
    "sns.heatmap(cm_matrix_rf_out_of_sample_40, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix for Random Forest: 40% Probability Cutoff')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601f82bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting ROC Curve\n",
    "\n",
    "fpr_rf, tpr_rf, _ = roc_curve(y_new_test, y_proba_out_of_sample['RandomForest'], sample_weight=weight_test)\n",
    "roc_auc_rf = auc(fpr_rf, tpr_rf)\n",
    "\n",
    "fpr_lr, tpr_lr, _ = roc_curve(y_new_test, y_proba_out_of_sample['LogisticRegression'], sample_weight=weight_test)\n",
    "roc_auc_lr = auc(fpr_lr, tpr_lr)\n",
    "\n",
    "fpr_gb, tpr_gb, _ = roc_curve(y_new_test, y_proba_out_of_sample['GradientBoosting'], sample_weight=weight_test)\n",
    "roc_auc_gb = auc(fpr_gb, tpr_gb)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "plt.plot(fpr_rf, tpr_rf, color='blue', lw=2, label=f'Random Forest (AUC = {roc_auc_rf:.2f})')\n",
    "plt.plot(fpr_lr, tpr_lr, color='orange', lw=2, label=f'Logistic Regression (AUC = {roc_auc_lr:.2f})')\n",
    "plt.plot(fpr_gb, tpr_gb, color='green', lw=2, label=f'Gradient Boosting (AUC = {roc_auc_gb:.2f})')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], color='red', lw=1, linestyle='--', label='Random Guess')\n",
    "\n",
    "plt.title('ROC Curve Comparison', fontsize=16)\n",
    "plt.xlabel('False Positive Rate', fontsize=14)\n",
    "plt.ylabel('True Positive Rate', fontsize=14)\n",
    "plt.legend(loc='lower right', fontsize=12, frameon=True, shadow=True)\n",
    "plt.grid(True, linestyle='--', lw=0.5, alpha=0.7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23770ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Most important features Logistic regression\n",
    "\n",
    "lr_most_important = feature_importances_out_of_sample['LogisticRegression']\n",
    "feature_importances_lr = {feature: abs(coef) for feature, coef in lr_most_important.items()}\n",
    "\n",
    "sorted_features = sorted(feature_importances_lr.items(), key=lambda item: item[1], reverse=True)\n",
    "\n",
    "top_20_features = sorted_features[:20]\n",
    "\n",
    "top_features_df = pd.DataFrame(top_20_features, columns=['Feature', 'Importance'])\n",
    "\n",
    "top_features_df.sort_values(by='Importance', ascending=False, inplace=True)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.barplot(x='Importance', y='Feature', data=top_features_df, palette='viridis')\n",
    "\n",
    "plt.xlabel('Logistic Regression Coefficient Magnitude')\n",
    "plt.ylabel('Feature')\n",
    "plt.title('Top 20 Features for Logistic Regression Model')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
